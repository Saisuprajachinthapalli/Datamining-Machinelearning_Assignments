{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtqJc5i0cFh7",
        "outputId": "b87d8fe8-1cf0-409f-d242-bbd319ce9a74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Accuracy with L2 regularization: 1.0\n",
            "Accuracy using sklearn: 1.0\n"
          ]
        }
      ],
      "source": [
        "# 1) complete the following code of logistic regression\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "alpha = 0.01\n",
        "num_iter = 1000\n",
        "\n",
        "X, y = make_classification(\n",
        "        n_samples=100, n_features=2, n_redundant=0,\n",
        "        n_informative=2, random_state=1, n_clusters_per_class=1 )\n",
        "\n",
        "def add_ones(X):\n",
        "   ones = np.ones((X.shape[0], 1))\n",
        "   return np.hstack((ones, X))\n",
        "\n",
        "def comp_p(X, w):\n",
        "   return 1 / (1 + np.exp(-np.dot(X, w)))\n",
        "\n",
        "X_1 = add_ones(X)\n",
        "\n",
        "# initialize w=0\n",
        "w = np.zeros(X_1.shape[1])\n",
        "m = y.size\n",
        "\n",
        "for i in range(num_iter):\n",
        "\n",
        "    p = comp_p(X_1, w)\n",
        "    gradient = np.dot(X_1.T, (y - p)) / m\n",
        "    w = w + alpha * gradient\n",
        "\n",
        "prediction_probabilities = comp_p(X_1, w)\n",
        "\n",
        "class_values = (prediction_probabilities >= 0.5).astype(int)\n",
        "\n",
        "accuracy = np.mean(class_values == y)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# 2) define the regularization strength\n",
        "\n",
        "lambd = 0.1\n",
        "\n",
        "for i in range(num_iter):\n",
        "\n",
        "    p = comp_p(X_1, w)\n",
        "    gradient = np.dot(X_1.T, (y - p)) / m - lambd * w\n",
        "    w = w + alpha * gradient\n",
        "\n",
        "prediction_probabilities = comp_p(X_1, w)\n",
        "\n",
        "class_values = (prediction_probabilities >= 0.5).astype(int)\n",
        "\n",
        "accuracy = np.mean(class_values == y)\n",
        "print(\"Accuracy with L2 regularization:\", accuracy)\n",
        "\n",
        "# 3) [bonus 6 pts] Implement using sklearn\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "log_reg_sklearn = LogisticRegression(penalty='l2', C=1/lambd)\n",
        "\n",
        "log_reg_sklearn.fit(X, y)\n",
        "\n",
        "prediction_probabilities_sklearn = log_reg_sklearn.predict_proba(X)[:, 1]\n",
        "\n",
        "class_values_sklearn = (prediction_probabilities_sklearn >= 0.5).astype(int)\n",
        "\n",
        "accuracy_sklearn = log_reg_sklearn.score(X, y)\n",
        "print(\"Accuracy using sklearn:\", accuracy_sklearn)\n"
      ]
    }
  ]
}