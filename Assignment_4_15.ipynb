{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fErqCDlu5fX_",
        "outputId": "98ad8985-3d1d-4711-b03c-46d6dfe1e045"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: x = 9.9776, y = -10.0224, f(x, y) = 109.4415\n",
            "Iteration 10: x = 9.8561, y = -10.1442, f(x, y) = 106.4217\n",
            "Iteration 20: x = 9.7548, y = -10.2458, f(x, y) = 103.9291\n",
            "Iteration 30: x = 9.6550, y = -10.3459, f(x, y) = 101.4920\n",
            "Iteration 40: x = 9.5553, y = -10.4460, f(x, y) = 99.0785\n",
            "Iteration 50: x = 9.4557, y = -10.5460, f(x, y) = 96.6853\n",
            "Iteration 60: x = 9.3561, y = -10.6460, f(x, y) = 94.3121\n",
            "Iteration 70: x = 9.2564, y = -10.7460, f(x, y) = 91.9588\n",
            "Iteration 80: x = 9.1568, y = -10.8460, f(x, y) = 89.6254\n",
            "Iteration 90: x = 9.0572, y = -10.9460, f(x, y) = 87.3120\n",
            "Final x = 8.9675, Final y = -11.0360, Final f(x, y) = 85.2469\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the function and its gradients\n",
        "def f(x, y):\n",
        "    return x**2 + 2*y +3*x\n",
        "\n",
        "# Compute the derivative with respect to x\n",
        "def f_der_x(x):\n",
        "     return 2*x + 3\n",
        "\n",
        "# Compute the derivative with respect to y\n",
        "def f_der_y(y):\n",
        "     return 2\n",
        "\n",
        "# Implement RMSProp\n",
        "\n",
        "# RMSProp parameters\n",
        "# : gamma\n",
        "# : lrate\n",
        "# : epsilon\n",
        "gamma = 0.8\n",
        "lrate = 0.01\n",
        "epsilon = 1e-8\n",
        "\n",
        "# Accumulated gradient squares for x and y, respectively\n",
        "# : gt_x and gt_y\n",
        "gt_x = 0\n",
        "gt_y = 0\n",
        "\n",
        "# Initial values of x and y\n",
        "x, y = 10.0, -10.0\n",
        "no_iterations = 100\n",
        "\n",
        "# RMSProp optimization loop\n",
        "for i in range(no_iterations):\n",
        "    g_x = f_der_x(x)\n",
        "    g_y = f_der_y(y)\n",
        "\n",
        "    # Update accumulated squared gradients for x and y\n",
        "    gt_x = gamma * gt_x + (1 - gamma) * g_x**2\n",
        "    gt_y = gamma * gt_y + (1 - gamma) * g_y**2\n",
        "\n",
        "    # Update variables x and y using RMSProp rule\n",
        "    x = x - (lrate * g_x) / (np.sqrt(gt_x) + epsilon)\n",
        "    y = y - (lrate * g_y) / (np.sqrt(gt_y) + epsilon)\n",
        "\n",
        "    # Optionally print the values of x, y, and the function value to track progress\n",
        "    if i % 10 == 0:\n",
        "        print(f\"Iteration {i}: x = {x:.4f}, y = {y:.4f}, f(x, y) = {f(x, y):.4f}\")\n",
        "\n",
        "# Show the final x and f(x, y) value\n",
        "print(f\"Final x = {x:.4f}, Final y = {y:.4f}, Final f(x, y) = {f(x, y):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) [bonus 4 pts] Add L2 regularization. You have to modify f, f_der_x, f-der_y functions.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Define the function with L2 regularization\n",
        "def f(x, y, lambda_reg=0.1):\n",
        "    return x**2 + 2*y + 3*x + lambda_reg * (x**2 + y**2)\n",
        "\n",
        "# Compute the derivative with respect to x (including L2 regularization term)\n",
        "def f_der_x(x, lambda_reg=0.1):\n",
        "    return 2*x + 3 + 2*lambda_reg * x\n",
        "\n",
        "# Compute the derivative with respect to y (including L2 regularization term)\n",
        "def f_der_y(y, lambda_reg=0.1):\n",
        "    return 2 + 2*lambda_reg * y\n",
        "\n",
        "# RMSProp parameters\n",
        "gamma = 0.8  # Decay factor\n",
        "lrate = 0.01  # Learning rate\n",
        "epsilon = 1e-8  # Small value to prevent division by zero\n",
        "lambda_reg = 0.1  # L2 regularization coefficient\n",
        "\n",
        "# Accumulated gradient squares for x and y, respectively\n",
        "gt_x = 0\n",
        "gt_y = 0\n",
        "\n",
        "# Initial values of x and y\n",
        "x, y = 10.0, -10.0\n",
        "no_iterations = 100\n",
        "\n",
        "# RMSProp optimization loop with L2 regularization\n",
        "for i in range(no_iterations):\n",
        "    # Compute gradients (derivatives) for x and y\n",
        "    g_x = f_der_x(x, lambda_reg)\n",
        "    g_y = f_der_y(y, lambda_reg)\n",
        "\n",
        "    # Update accumulated squared gradients for x and y\n",
        "    gt_x = gamma * gt_x + (1 - gamma) * g_x**2\n",
        "    gt_y = gamma * gt_y + (1 - gamma) * g_y**2\n",
        "\n",
        "    # Update variables x and y using RMSProp rule\n",
        "    x = x - (lrate * g_x) / (np.sqrt(gt_x) + epsilon)\n",
        "    y = y - (lrate * g_y) / (np.sqrt(gt_y) + epsilon)\n",
        "\n",
        "    # Show progress: print every 10 iterations\n",
        "    if i % 10 == 0:\n",
        "        print(f\"Iteration = {i + 1}: x = {x:.4f}, y = {y:.4f}, f(x, y) = {f(x, y, lambda_reg):.4f}\")\n",
        "\n",
        "# Show the final x and f(x, y) value\n",
        "print(f\"Final x = {x:.4f}, Final y = {y:.4f}, Final f(x, y) = {f(x, y, lambda_reg):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfdsFuyC6lwX",
        "outputId": "478ae85a-5e05-44a4-c14b-3235ee950949"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration = 1: x = 9.9776, y = -10.0000, f(x, y) = 129.4415\n",
            "Iteration = 11: x = 9.8561, y = -10.0000, f(x, y) = 126.4243\n",
            "Iteration = 21: x = 9.7548, y = -10.0000, f(x, y) = 123.9365\n",
            "Iteration = 31: x = 9.6550, y = -10.0000, f(x, y) = 121.5060\n",
            "Iteration = 41: x = 9.5553, y = -10.0000, f(x, y) = 119.1012\n",
            "Iteration = 51: x = 9.4557, y = -10.0000, f(x, y) = 116.7187\n",
            "Iteration = 61: x = 9.3561, y = -10.0000, f(x, y) = 114.3582\n",
            "Iteration = 71: x = 9.2565, y = -10.0000, f(x, y) = 112.0195\n",
            "Iteration = 81: x = 9.1568, y = -10.0000, f(x, y) = 109.7028\n",
            "Iteration = 91: x = 9.0572, y = -10.0000, f(x, y) = 107.4080\n",
            "Final x = 8.9676, Final y = -10.0000, Final f(x, y) = 105.3614\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) [bonus 8 pts] Implement AdaDelta. You have to define the following  and modify the update rule.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Define the function with L2 regularization\n",
        "def f(x, y, lambda_reg=0.1):\n",
        "    return x**2 + 2*y + 3*x + lambda_reg * (x**2 + y**2)\n",
        "\n",
        "# Compute the derivative with respect to x (including L2 regularization term)\n",
        "def f_der_x(x, lambda_reg=0.1):\n",
        "    return 2*x + 3 + 2*lambda_reg * x\n",
        "\n",
        "# Compute the derivative with respect to y (including L2 regularization term)\n",
        "def f_der_y(y, lambda_reg=0.1):\n",
        "    return 2 + 2*lambda_reg * y\n",
        "\n",
        "# AdaDelta parameters\n",
        "gamma = 0.9  # Decay factor\n",
        "epsilon = 1e-8  # Small value to prevent division by zero\n",
        "lambda_reg = 0.1  # L2 regularization coefficient\n",
        "\n",
        "# Accumulated gradient squares for x and y, respectively\n",
        "E_g2_x = 0\n",
        "E_g2_y = 0\n",
        "\n",
        "# Accumulated update squares for x and y\n",
        "E_delta2_x = 0\n",
        "E_delta2_y = 0\n",
        "\n",
        "# Initial values of x and y\n",
        "x, y = 10.0, -10.0\n",
        "no_iterations = 100\n",
        "\n",
        "# AdaDelta optimization loop with L2 regularization\n",
        "for i in range(no_iterations):\n",
        "    # Compute gradients (derivatives) for x and y\n",
        "    g_x = f_der_x(x, lambda_reg)\n",
        "    g_y = f_der_y(y, lambda_reg)\n",
        "\n",
        "    # Update accumulated squared gradients for x and y\n",
        "    E_g2_x = gamma * E_g2_x + (1 - gamma) * g_x**2\n",
        "    E_g2_y = gamma * E_g2_y + (1 - gamma) * g_y**2\n",
        "\n",
        "    # Compute update values using AdaDelta rule\n",
        "    delta_x = - (np.sqrt(E_delta2_x + epsilon) / np.sqrt(E_g2_x + epsilon)) * g_x\n",
        "    delta_y = - (np.sqrt(E_delta2_y + epsilon) / np.sqrt(E_g2_y + epsilon)) * g_y\n",
        "\n",
        "    # Update accumulated squared parameter updates\n",
        "    E_delta2_x = gamma * E_delta2_x + (1 - gamma) * delta_x**2\n",
        "    E_delta2_y = gamma * E_delta2_y + (1 - gamma) * delta_y**2\n",
        "\n",
        "    # Update variables x and y using AdaDelta rule\n",
        "    x += delta_x\n",
        "    y += delta_y\n",
        "\n",
        "    # Show progress: print every 10 iterations\n",
        "    if i % 10 == 0:\n",
        "        print(f\"Iteration = {i + 1}: x = {x:.4f}, y = {y:.4f}, f(x, y) = {f(x, y, lambda_reg):.4f}\")\n",
        "\n",
        "# Show the final x and f(x, y) value\n",
        "print(f\"Final x = {x:.4f}, Final y = {y:.4f}, Final f(x, y) = {f(x, y, lambda_reg):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQ9HDlmj6_xm",
        "outputId": "8738e6c5-91b4-489d-aa42-22c68ce079f8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration = 1: x = 9.9997, y = -10.0000, f(x, y) = 129.9921\n",
            "Iteration = 11: x = 9.9963, y = -10.0000, f(x, y) = 129.9068\n",
            "Iteration = 21: x = 9.9926, y = -10.0000, f(x, y) = 129.8162\n",
            "Iteration = 31: x = 9.9889, y = -10.0000, f(x, y) = 129.7218\n",
            "Iteration = 41: x = 9.9850, y = -10.0000, f(x, y) = 129.6241\n",
            "Iteration = 51: x = 9.9809, y = -10.0000, f(x, y) = 129.5232\n",
            "Iteration = 61: x = 9.9768, y = -10.0000, f(x, y) = 129.4194\n",
            "Iteration = 71: x = 9.9725, y = -10.0000, f(x, y) = 129.3127\n",
            "Iteration = 81: x = 9.9681, y = -10.0000, f(x, y) = 129.2033\n",
            "Iteration = 91: x = 9.9636, y = -10.0000, f(x, y) = 129.0910\n",
            "Final x = 9.9594, Final y = -10.0000, Final f(x, y) = 128.9878\n"
          ]
        }
      ]
    }
  ]
}